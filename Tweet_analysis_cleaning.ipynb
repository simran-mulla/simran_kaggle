{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d87510-2e06-4a18-92fa-b8832b715805",
   "metadata": {},
   "source": [
    "#### \n",
    "In the below project the cleaning of the tweets from text column , tokenization of tweets and removal of stop words has been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227b7411-e2b6-4fd5-bd37-891849e3f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15aec84-1718-4412-9ae0-f2eace35a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"train.csv\")\n",
    "df_test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746cb0f3-d07f-41b4-9bf9-3eb197c73f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n",
      "(3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bd815-98f7-4a01-9e3a-1de53746da55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16fe998-2ff9-4d87-b4bb-938cde9f89a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>5673</td>\n",
       "      <td>floods</td>\n",
       "      <td>london</td>\n",
       "      <td>there's this person &amp;amp; they reckon when you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>6104</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>Denver, Colorado</td>\n",
       "      <td>(Also I dont think sewing thought a leather be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>6735</td>\n",
       "      <td>lava</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>A river of lava in the sky this evening! It wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>5056</td>\n",
       "      <td>eyewitness</td>\n",
       "      <td>Orlando, Fl</td>\n",
       "      <td>WFTV Eyewitness News: FBI: Man who stole US se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>484</td>\n",
       "      <td>armageddon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Christians United for #Israel (#CUFI): Jews s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>1937</td>\n",
       "      <td>burning%20buildings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7431</th>\n",
       "      <td>10629</td>\n",
       "      <td>wounded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National free root beer float day is tomorrow ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              keyword          location  \\\n",
       "3994   5673               floods            london   \n",
       "4298   6104             hellfire  Denver, Colorado   \n",
       "4736   6735                 lava      San Jose, CA   \n",
       "3536   5056           eyewitness       Orlando, Fl   \n",
       "337     484           armageddon               NaN   \n",
       "1339   1937  burning%20buildings               NaN   \n",
       "7431  10629              wounded               NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "3994  there's this person &amp; they reckon when you...       0  \n",
       "4298  (Also I dont think sewing thought a leather be...       0  \n",
       "4736  A river of lava in the sky this evening! It wa...       0  \n",
       "3536  WFTV Eyewitness News: FBI: Man who stole US se...       0  \n",
       "337   #Christians United for #Israel (#CUFI): Jews s...       0  \n",
       "1339  @foxnewsvideo @AIIAmericanGirI @ANHQDC So ... ...       1  \n",
       "7431  National free root beer float day is tomorrow ...       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>9622</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>Lake Hefner Bike Trail</td>\n",
       "      <td>Stood there for 20 mics tryin to get a #lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>8160</td>\n",
       "      <td>rescuers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fears over missing migrants in Med: Rescuers s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>2977</td>\n",
       "      <td>dead</td>\n",
       "      <td>My World</td>\n",
       "      <td>Ross better not be dead! #Emmerdale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>5472</td>\n",
       "      <td>flames</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@xodeemorgss she went to get rice and the whol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>7429</td>\n",
       "      <td>obliterated</td>\n",
       "      <td>Calgary, Alberta, Canada</td>\n",
       "      <td>Disgusting! Drunk Meals 101: What To Cook When...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>7509</td>\n",
       "      <td>oil%20spill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PODCAST: Oil spill anniversary http://t.co/wVd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>6367</td>\n",
       "      <td>hostages</td>\n",
       "      <td>Hubli, Karnataka</td>\n",
       "      <td>@IndiaToday @Iamtssudhir Time for India to joi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword                  location  \\\n",
       "2903  9622  thunderstorm    Lake Hefner Bike Trail   \n",
       "2442  8160      rescuers                       NaN   \n",
       "902   2977          dead                  My World   \n",
       "1626  5472        flames                       NaN   \n",
       "2226  7429   obliterated  Calgary, Alberta, Canada   \n",
       "2258  7509   oil%20spill                       NaN   \n",
       "1890  6367      hostages          Hubli, Karnataka   \n",
       "\n",
       "                                                   text  \n",
       "2903  Stood there for 20 mics tryin to get a #lightn...  \n",
       "2442  Fears over missing migrants in Med: Rescuers s...  \n",
       "902                 Ross better not be dead! #Emmerdale  \n",
       "1626  @xodeemorgss she went to get rice and the whol...  \n",
       "2226  Disgusting! Drunk Meals 101: What To Cook When...  \n",
       "2258  PODCAST: Oil spill anniversary http://t.co/wVd...  \n",
       "1890  @IndiaToday @Iamtssudhir Time for India to joi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.sample(7))\n",
    "print()\n",
    "display(df_test.sample(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2c5914-335b-4481-bbe2-dd36b987a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Train Data:\n",
      "                                                text  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...   \n",
      "1             Forest fire near La Ronge Sask. Canada   \n",
      "2  All residents asked to 'shelter in place' are ...   \n",
      "3  13,000 people receive #wildfires evacuation or...   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "                                          text_clean  \n",
      "0  our deeds are the reason of this earthquake ma...  \n",
      "1              forest fire near la ronge sask canada  \n",
      "2  all residents asked to shelter in place are be...  \n",
      "3  13000 people receive wildfires evacuation orde...  \n",
      "4  just got sent this photo from ruby alaska as s...  \n",
      "------------------------------------------------------------------------------------------\n",
      "Cleaned Test Data:\n",
      "                                                text  \\\n",
      "0                 Just happened a terrible car crash   \n",
      "1  Heard about #earthquake is different cities, s...   \n",
      "2  there is a forest fire at spot pond, geese are...   \n",
      "3           Apocalypse lighting. #Spokane #wildfires   \n",
      "4      Typhoon Soudelor kills 28 in China and Taiwan   \n",
      "\n",
      "                                          text_clean  \n",
      "0                 just happened a terrible car crash  \n",
      "1  heard about earthquake is different cities sta...  \n",
      "2  there is a forest fire at spot pond geese are ...  \n",
      "3              apocalypse lighting spokane wildfires  \n",
      "4      typhoon soudelor kills 28 in china and taiwan  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(\n",
    "        r'['\n",
    "        u'\\U0001F600-\\U0001F64F'\n",
    "        u'\\U0001F300-\\U0001F5FF'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\U0001F1E0-\\U0001F1FF'\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+', \n",
    "        '', \n",
    "        text\n",
    "    )\n",
    "    text = re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "df_train['text_clean'] = df_train['text'].apply(clean_text)\n",
    "df_test['text_clean'] = df_test['text'].apply(clean_text)\n",
    "\n",
    "print(\"Cleaned Train Data:\")\n",
    "print(df_train[['text', 'text_clean']].head())\n",
    "\n",
    "print(\"---\" * 30)\n",
    "\n",
    "print(\"Cleaned Test Data:\")\n",
    "print(df_test[['text', 'text_clean']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef54b92-222b-4c46-9353-5f935707295f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "keyword         61\n",
       "location      2533\n",
       "text             0\n",
       "target           0\n",
       "text_clean       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b0526e-8f3c-4e9e-87c3-e7a6139a8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5b88f0-9ad6-466a-8d61-28df6d9d2ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b1d7d59-a788-4b73-a1bb-0691bd94dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##we will tokenize the 'text_clean' column\n",
    "\n",
    "df_train['tokenized']=df_train['text_clean'].apply(word_tokenize)\n",
    "df_test['tokenized']=df_test['text_clean'].apply(word_tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "617fe237-13d9-4966-8d79-4b6944b4ced7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Tokenized:\n",
      "                                          text_clean  \\\n",
      "0  our deeds are the reason of this earthquake ma...   \n",
      "1              forest fire near la ronge sask canada   \n",
      "2  all residents asked to shelter in place are be...   \n",
      "3  13000 people receive wildfires evacuation orde...   \n",
      "4  just got sent this photo from ruby alaska as s...   \n",
      "\n",
      "                                           tokenized  \n",
      "0  [our, deeds, are, the, reason, of, this, earth...  \n",
      "1      [forest, fire, near, la, ronge, sask, canada]  \n",
      "2  [all, residents, asked, to, shelter, in, place...  \n",
      "3  [13000, people, receive, wildfires, evacuation...  \n",
      "4  [just, got, sent, this, photo, from, ruby, ala...  \n",
      "------------------------------------------------------------------------------------------\n",
      "Test Data Tokenized:\n",
      "                                          text_clean  \\\n",
      "0                 just happened a terrible car crash   \n",
      "1  heard about earthquake is different cities sta...   \n",
      "2  there is a forest fire at spot pond geese are ...   \n",
      "3              apocalypse lighting spokane wildfires   \n",
      "4      typhoon soudelor kills 28 in china and taiwan   \n",
      "\n",
      "                                           tokenized  \n",
      "0          [just, happened, a, terrible, car, crash]  \n",
      "1  [heard, about, earthquake, is, different, citi...  \n",
      "2  [there, is, a, forest, fire, at, spot, pond, g...  \n",
      "3         [apocalypse, lighting, spokane, wildfires]  \n",
      "4  [typhoon, soudelor, kills, 28, in, china, and,...  \n"
     ]
    }
   ],
   "source": [
    "# Display tokenized results\n",
    "print(\"Train Data Tokenized:\")\n",
    "print(df_train[['text_clean', 'tokenized']].head())\n",
    "\n",
    "print(\"---\" * 30)\n",
    "\n",
    "print(\"Test Data Tokenized:\")\n",
    "print(df_test[['text_clean', 'tokenized']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "480e5f45-a1d9-4e66-9064-380c4028dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.find('tokenizers/punkt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9d948a0-b026-4515-a164-1e345e73a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     D:/Brainworks/Projects/Tweet_analysis/venv...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt', download_dir=\"D:/Brainworks/Projects/Tweet_analysis/venv\")\n",
    "import nltk\n",
    "nltk.data.path.append(\"D:/Brainworks/Projects/Tweet_analysis/venv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "799c61b4-8104-4e2a-a210-e50105efc9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7074</th>\n",
       "      <td>10132</td>\n",
       "      <td>upheaval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acquire your postexistence straight a elevatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>acquire your postexistence straight a elevatio...</td>\n",
       "      <td>[acquire, your, postexistence, straight, a, el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>774</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Jersey City, New Jersey</td>\n",
       "      <td>Musician Kalle Mattson Recreates 34 Classic Al...</td>\n",
       "      <td>0</td>\n",
       "      <td>musician kalle mattson recreates 34 classic al...</td>\n",
       "      <td>[musician, kalle, mattson, recreates, 34, clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>2651</td>\n",
       "      <td>crashed</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>MH370: Intact part lifts odds plane glided not...</td>\n",
       "      <td>1</td>\n",
       "      <td>mh370 intact part lifts odds plane glided not ...</td>\n",
       "      <td>[mh370, intact, part, lifts, odds, plane, glid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>98</td>\n",
       "      <td>accident</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Accident center lane blocked in #SantaClara on...</td>\n",
       "      <td>1</td>\n",
       "      <td>accident center lane blocked in santaclara on ...</td>\n",
       "      <td>[accident, center, lane, blocked, in, santacla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>5520</td>\n",
       "      <td>flattened</td>\n",
       "      <td>Frome, Somerset, England</td>\n",
       "      <td>Zouma has just absolutely flattened that guy ??</td>\n",
       "      <td>0</td>\n",
       "      <td>zouma has just absolutely flattened that guy</td>\n",
       "      <td>[zouma, has, just, absolutely, flattened, that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    keyword                  location  \\\n",
       "7074  10132   upheaval                       NaN   \n",
       "532     774  avalanche   Jersey City, New Jersey   \n",
       "1844   2651    crashed              Buenos Aires   \n",
       "69       98   accident           Santa Clara, CA   \n",
       "3882   5520  flattened  Frome, Somerset, England   \n",
       "\n",
       "                                                   text  target  \\\n",
       "7074  Acquire your postexistence straight a elevatio...       0   \n",
       "532   Musician Kalle Mattson Recreates 34 Classic Al...       0   \n",
       "1844  MH370: Intact part lifts odds plane glided not...       1   \n",
       "69    Accident center lane blocked in #SantaClara on...       1   \n",
       "3882    Zouma has just absolutely flattened that guy ??       0   \n",
       "\n",
       "                                             text_clean  \\\n",
       "7074  acquire your postexistence straight a elevatio...   \n",
       "532   musician kalle mattson recreates 34 classic al...   \n",
       "1844  mh370 intact part lifts odds plane glided not ...   \n",
       "69    accident center lane blocked in santaclara on ...   \n",
       "3882       zouma has just absolutely flattened that guy   \n",
       "\n",
       "                                              tokenized  \n",
       "7074  [acquire, your, postexistence, straight, a, el...  \n",
       "532   [musician, kalle, mattson, recreates, 34, clas...  \n",
       "1844  [mh370, intact, part, lifts, odds, plane, glid...  \n",
       "69    [accident, center, lane, blocked, in, santacla...  \n",
       "3882  [zouma, has, just, absolutely, flattened, that...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>6016</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#hot #fiat Olap #world pres: http://t.co/QddYw...</td>\n",
       "      <td>hot fiat olap world pres how to recognize a ha...</td>\n",
       "      <td>[hot, fiat, olap, world, pres, how, to, recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>6044</td>\n",
       "      <td>heat%20wave</td>\n",
       "      <td>For a healthier, happier YOU!</td>\n",
       "      <td>Never really seen olive leaf extract to help w...</td>\n",
       "      <td>never really seen olive leaf extract to help w...</td>\n",
       "      <td>[never, really, seen, olive, leaf, extract, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>8263</td>\n",
       "      <td>rioting</td>\n",
       "      <td>Azeroth</td>\n",
       "      <td>@amiestager there's better alternatives than r...</td>\n",
       "      <td>amiestager theres better alternatives than rio...</td>\n",
       "      <td>[amiestager, theres, better, alternatives, tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>fuck off</td>\n",
       "      <td>[fuck, off]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>7406</td>\n",
       "      <td>obliterated</td>\n",
       "      <td>Nanda Parbat</td>\n",
       "      <td>Mad kids and innocent people who had nothing t...</td>\n",
       "      <td>mad kids and innocent people who had nothing t...</td>\n",
       "      <td>[mad, kids, and, innocent, people, who, had, n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword                       location  \\\n",
       "1781  6016    hazardous                            NaN   \n",
       "1790  6044  heat%20wave  For a healthier, happier YOU!   \n",
       "2471  8263      rioting                        Azeroth   \n",
       "9       29          NaN                            NaN   \n",
       "2218  7406  obliterated                   Nanda Parbat   \n",
       "\n",
       "                                                   text  \\\n",
       "1781  #hot #fiat Olap #world pres: http://t.co/QddYw...   \n",
       "1790  Never really seen olive leaf extract to help w...   \n",
       "2471  @amiestager there's better alternatives than r...   \n",
       "9                                             Fuck off!   \n",
       "2218  Mad kids and innocent people who had nothing t...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "1781  hot fiat olap world pres how to recognize a ha...   \n",
       "1790  never really seen olive leaf extract to help w...   \n",
       "2471  amiestager theres better alternatives than rio...   \n",
       "9                                              fuck off   \n",
       "2218  mad kids and innocent people who had nothing t...   \n",
       "\n",
       "                                              tokenized  \n",
       "1781  [hot, fiat, olap, world, pres, how, to, recogn...  \n",
       "1790  [never, really, seen, olive, leaf, extract, to...  \n",
       "2471  [amiestager, theres, better, alternatives, tha...  \n",
       "9                                           [fuck, off]  \n",
       "2218  [mad, kids, and, innocent, people, who, had, n...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.sample(5))\n",
    "print()\n",
    "display(df_test.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7e1022-db1c-4368-9802-68d59e7936e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###we will do lowercase,Stopword removal, POS tagging, and Wordnet COnversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec2f0c9-6f45-4627-8b97-22a7207abefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           tokenized  \\\n",
      "0  [our, deeds, are, the, reason, of, this, earth...   \n",
      "1      [forest, fire, near, la, ronge, sask, canada]   \n",
      "2  [all, residents, asked, to, shelter, in, place...   \n",
      "3  [13000, people, receive, wildfires, evacuation...   \n",
      "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
      "\n",
      "                                               lower  \n",
      "0  [our, deeds, are, the, reason, of, this, earth...  \n",
      "1      [forest, fire, near, la, ronge, sask, canada]  \n",
      "2  [all, residents, asked, to, shelter, in, place...  \n",
      "3  [13000, people, receive, wildfires, evacuation...  \n",
      "4  [just, got, sent, this, photo, from, ruby, ala...  \n"
     ]
    }
   ],
   "source": [
    "df_train['lower']=df_train['tokenized'].map(lambda tokens:[word.lower() for word in tokens])\n",
    "\n",
    "##display the first few rows of processed data\n",
    "print(df_train[['tokenized','lower']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "713d25ef-bba8-4f8e-b13e-d99802a12168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                               lower  \n",
       "0  [our, deeds, are, the, reason, of, this, earth...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [all, residents, asked, to, shelter, in, place...  \n",
       "3  [13000, people, receive, wildfires, evacuation...  \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5e1371a-8e40-4329-aea0-1d2956ab17ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               lower  \\\n",
      "0  [our, deeds, are, the, reason, of, this, earth...   \n",
      "1      [forest, fire, near, la, ronge, sask, canada]   \n",
      "2  [all, residents, asked, to, shelter, in, place...   \n",
      "3  [13000, people, receive, wildfires, evacuation...   \n",
      "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
      "\n",
      "                                   stopwords_removed  \n",
      "0  [deeds, reason, earthquake, may, allah, forgiv...  \n",
      "1      [forest, fire, near, la, ronge, sask, canada]  \n",
      "2  [residents, asked, shelter, place, notified, o...  \n",
      "3  [13000, people, receive, wildfires, evacuation...  \n",
      "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "##Ensure stopwords are available\n",
    "nltk.download('stopwords')\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "###Remove stopwords from the 'lower' column\n",
    "df_train['stopwords_removed']=df_train['lower'].map(lambda tokens:[word for word in tokens if word not in stop])\n",
    "\n",
    "##display the first few rows with stopwords removed\n",
    "print(df_train[['lower','stopwords_removed']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca638bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>stopwords_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [residents, asked, shelter, place, notified, o...  \n",
       "3  [13000, people, receive, wildfires, evacuation...  \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
